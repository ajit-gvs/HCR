{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO76l9bfNHG98bha6HFWGdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajit-gvs/HCR/blob/master/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJzQTbP2NQlN"
      },
      "source": [
        "import cv2\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from PIL import Image\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "from PIL import ImageFile\r\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n",
        "directory='/content/data'\r\n",
        "text=open('/content/words_new.txt')\r\n",
        "\r\n",
        "samples,labels=data_label()\r\n",
        "trainx,testx,trainy,testy=data_split(samples,labels):\r\n",
        "test_accuracy,train_accuracy,parameters=model(trainx,trainy,testx,testy)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_bSLjwcN_Ul"
      },
      "source": [
        "def data_label():\r\n",
        "  labels=[]\r\n",
        "  samples=[]\r\n",
        " \r\n",
        "\r\n",
        " \r\n",
        "  \r\n",
        "  for filename in os.listdir(directory):\r\n",
        "    image_orig=Image.open('/content/data/'+filename,'r')\r\n",
        "    img=image_binarization(image_orig)\r\n",
        "    \r\n",
        "    \r\n",
        "    lines=line_segmentation(img)\r\n",
        "    \r\n",
        "    for i in range(len(lines)):\r\n",
        "      for line in text:\r\n",
        "        \r\n",
        "         \r\n",
        "        lineSplit = line.strip().split(\" \")\r\n",
        "        linesplit=lineSplit[0].split(\"-\")\r\n",
        "        string= linesplit[0]+'-'+linesplit[1]\r\n",
        "        if string==filename:\r\n",
        "          count=count+1\r\n",
        "    if count==len(lines): \r\n",
        "      for i in range(len(lines)):\r\n",
        "        words=word_segmentation(lines[i])\r\n",
        "        for line in text:\r\n",
        "          lineSplit = line.strip().split(' ')\r\n",
        "          filename_split=filename.split('.')\r\n",
        "          if lineSplit[0]==filename_split[0]+'-'+str(0)+str(i):\r\n",
        "            word_split = lineSplit[8].split('|')\r\n",
        "            if len(word_split)==len(words):\r\n",
        "              for j in range(len(words)):\r\n",
        "                characters=char_segmentation(words[j])\r\n",
        "                for k in range(len(characters)):\r\n",
        "                  if len(word_split[k])==len(characters):\r\n",
        "                    character=cv2.resize(characters[k],(28,28))\r\n",
        "                    samples.append(character)\r\n",
        "                    for l in word_split[k]:\r\n",
        "                      labels.append(l)\r\n",
        "    \r\n",
        "    \r\n",
        "                  \r\n",
        "  print(len(samples))\r\n",
        "  return  samples,labels\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def data_split(samples,labels):\r\n",
        "  samples = np.array(samples)\r\n",
        "  labels = np.array(labels)   \r\n",
        "  lb = LabelBinarizer()\r\n",
        "  LABELS = lb.fit_transform(labels) \r\n",
        "  trainx,testx,trainy,testy = train_test_split(samples, LABELS, test_size=0.2, random_state=42)\r\n",
        "\r\n",
        "  return trainx,testx,trainy,testy\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def image_binarization(image):\r\n",
        "  # converting image to grayscale\r\n",
        "  image_grayscale=image.convert('L')\r\n",
        "  img=np.array(image_grayscale)\r\n",
        "   \r\n",
        "  img[img <128] = 0\r\n",
        "  img[img >=128] = 254 \r\n",
        "\r\n",
        "  #making white as foreground pixels and black as background pixels\r\n",
        "  img[img==0]=255\r\n",
        "  img[img==254]=0\r\n",
        "  \r\n",
        "  return img\r\n",
        "\r\n",
        "\r\n",
        "def line_segmentation(img):\r\n",
        "  start_matrix=[]\r\n",
        "  end_matrix=[]\r\n",
        "\r\n",
        "  #matrix to get the start and end points of a line\r\n",
        "  \r\n",
        "  lines=[]\r\n",
        "  begin_matrix=[]\r\n",
        "  stop_matrix=[]\r\n",
        "  del_start_matrix=[]\r\n",
        "  del_end_matrix=[]\r\n",
        "\r\n",
        "  horizontal_hist = np.sum(img,axis=1,keepdims=True)/255\r\n",
        "  start_count=0\r\n",
        "\r\n",
        "  for i in range(len(horizontal_hist)):\r\n",
        "\r\n",
        "    if horizontal_hist[i]>0 and horizontal_hist[i-1]==0:\r\n",
        "      start_count+=1\r\n",
        "      start_matrix.append(i)\r\n",
        "\r\n",
        "    if horizontal_hist[i]==0 and start_count>0 and horizontal_hist[i-1]>0:\r\n",
        "      end_matrix.append(i)\r\n",
        "\r\n",
        "  \r\n",
        "  \r\n",
        "  if len(start_matrix)==len(end_matrix):\r\n",
        "    for i in range(len(start_matrix)):\r\n",
        "      if end_matrix[i]-start_matrix[i]<20:\r\n",
        "        del_start_matrix.append(i)\r\n",
        "        del_end_matrix.append(i)\r\n",
        "    for i in range(len(start_matrix)):\r\n",
        "      count=0\r\n",
        "      for j in range(len(del_start_matrix)):\r\n",
        "        if i==del_start_matrix[j]:\r\n",
        "          count=count+1\r\n",
        "      if count==0:\r\n",
        "        begin_matrix.append(start_matrix[i])\r\n",
        "    for i in range(len(end_matrix)):\r\n",
        "      count=0\r\n",
        "      for j in range(len(del_end_matrix)):\r\n",
        "        if i==del_end_matrix[j]:\r\n",
        "          count=count+1\r\n",
        "      if count==0:\r\n",
        "        stop_matrix.append(end_matrix[i])\r\n",
        "    \r\n",
        "    for i in range(len(begin_matrix)):\r\n",
        "      lines.append(img[begin_matrix[i]:stop_matrix[i],:])\r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "  \r\n",
        "  return lines\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def word_segmentation(img):\r\n",
        "  start_matrix=[]\r\n",
        "  end_matrix=[]\r\n",
        "\r\n",
        "  #matrix to get the start and end points of a word\r\n",
        "  dissection_matrix=[]\r\n",
        "  \r\n",
        "  words=[]\r\n",
        "  m,n=img.shape\r\n",
        "  \r\n",
        "  length=[]\r\n",
        "  vertical_hist = np.sum(img,axis=0,keepdims=True)/255\r\n",
        "  \r\n",
        "  \r\n",
        "  start_count=0\r\n",
        "  \r\n",
        "  \r\n",
        "  for i in range(len(vertical_hist[0])):\r\n",
        "    if vertical_hist[0][i]>0 and vertical_hist[0][i-1]==0:\r\n",
        "      start_count+=1\r\n",
        "      start_matrix.append(i)\r\n",
        "    if vertical_hist[0][i]==0 and start_count>0 and vertical_hist[0][i-1]>0:\r\n",
        "      end_matrix.append(i)\r\n",
        "  \r\n",
        "  \r\n",
        "  length_mag=0\r\n",
        "  for i in range(len(start_matrix)):\r\n",
        "    if i>0:\r\n",
        "      length_mag=(start_matrix[i]-end_matrix[i-1])\r\n",
        "      length.append(length_mag)\r\n",
        "\r\n",
        "  max=np.max(length)\r\n",
        "     \r\n",
        "  avg=max/3\r\n",
        "\r\n",
        "  dissection_matrix.append([start_matrix[0],end_matrix[0]])\r\n",
        "  j=0\r\n",
        "  for i in range(len(length)-1):\r\n",
        "    \r\n",
        "    if length[i]> avg:\r\n",
        "      dissection_matrix.append([start_matrix[i+1],end_matrix[i+1]])\r\n",
        "      j=j+1\r\n",
        "      \r\n",
        "    if length[i]<=avg:\r\n",
        "      dissection_matrix[j][1]=end_matrix[i+1]\r\n",
        "  for i in range(len(dissection_matrix)):\r\n",
        "    words.append(img[0:m,dissection_matrix[i][0]:dissection_matrix[i][1]])\r\n",
        "  \r\n",
        "  return words\r\n",
        "\r\n",
        "\r\n",
        "def char_segmentation(img):\r\n",
        "  start_matrix=[]\r\n",
        "  dissection_matrix=[]\r\n",
        "  delete_matrix=[]\r\n",
        "  address_matrix=[]\r\n",
        "  \r\n",
        "  \r\n",
        "  characters=[]\r\n",
        "  m,n=img.shape\r\n",
        "\r\n",
        "  vertical_hist = np.sum(img,axis=0,keepdims=True)/255\r\n",
        "  start_matrix.append(0)\r\n",
        "\r\n",
        "  for i in range(len(vertical_hist[0])):\r\n",
        "    if vertical_hist[0][i]<10:\r\n",
        "      start_matrix.append(i)\r\n",
        "\r\n",
        "  for i in range(len(start_matrix)-1):\r\n",
        "    if start_matrix[i+1]-start_matrix[i]<10:\r\n",
        "      delete_matrix.append(i)\r\n",
        "  \r\n",
        "  for i in range(len(start_matrix)):\r\n",
        "    count=0\r\n",
        "    for j in range(len(delete_matrix)):\r\n",
        "      if  i==delete_matrix[j]:\r\n",
        "        count=count+1\r\n",
        "    if count==0:\r\n",
        "      address_matrix.append(start_matrix[i])\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(len(address_matrix)-1):\r\n",
        "    dissection_matrix.append([address_matrix[i],address_matrix[i+1]])\r\n",
        "  for i in range(len(dissection_matrix)):\r\n",
        "    characters.append(img[0:m,dissection_matrix[i][0]:dissection_matrix[i][1]])\r\n",
        "    \r\n",
        "    \r\n",
        "  return characters\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def buildCNN(trainx,parameters):\r\n",
        "  \r\n",
        "   \r\n",
        "  W1 = parameters['W1']\r\n",
        "  W2 = parameters['W2']\r\n",
        "  W3 = parameters['W3']\r\n",
        "  W4 = parameters['W4']\r\n",
        "  W5 = parameters['W5']\r\n",
        "  #Convolution Layer1\r\n",
        "  C1 = tf.nn.conv2d(trainx,W1, strides = [1,1,1,1], padding = 'SAME')\r\n",
        "  #Activation Layer1\r\n",
        "  A1 = tf.nn.relu(C1)\r\n",
        "  #Batch Normalization Layer1\r\n",
        "  #BN1 = tf.nn.batch_normalization(A1)\r\n",
        "  #Max Pooling Layer1\r\n",
        "  P1 = tf.nn.max_pool(BN1, ksize = [1,3,3,1], strides = [1,3,3,1], padding = 'VALID')\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "  #Convolution Layer2\r\n",
        "  C2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\r\n",
        "  #Activation Layer2\r\n",
        "  A2 = tf.nn.relu(C2)\r\n",
        "  #Batch Normalization Layer2\r\n",
        "  #BN2 = tf..batch_normalization(A2)\r\n",
        "  #Max Pooling Layer2\r\n",
        "  P2 = tf.nn.max_pool(BN2, ksize = [1,3,3,1], strides = [1,3,3,1], padding = 'VALID')\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "  #Convolution Layer3\r\n",
        "  C3 = tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding = 'SAME')\r\n",
        "  #Activation Layer3\r\n",
        "  A3 = tf.nn.relu(C3)\r\n",
        "  #Batch Normalization Layer2\r\n",
        "  #BN3 = tf.batch_normalization(A3)\r\n",
        "  #Max Pooling Layer3\r\n",
        "  P3 = tf.nn.max_pool(BN3, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\r\n",
        "\r\n",
        "\r\n",
        "    #Convolution Layer4\r\n",
        "  C4 = tf.nn.conv2d(P3,W4, strides = [1,1,1,1], padding = 'SAME')\r\n",
        "  #Activation Layer4\r\n",
        "  A4 = tf.nn.relu(C4)\r\n",
        "  #Batch Normalization Layer4\r\n",
        "  #BN4 = tf.batch_normalization(A4)\r\n",
        "  #Max Pooling Layer4\r\n",
        "  P4 = tf.nn.max_pool(BN4, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\r\n",
        "\r\n",
        "\r\n",
        "    #Convolution Layer5\r\n",
        "  C5 = tf.nn.conv2d(P4,W5, strides = [1,1,1,1], padding = 'SAME')\r\n",
        "  #Activation Layer5\r\n",
        "  A5 = tf.nn.relu(C5)\r\n",
        "  #Batch Normalization Layer2\r\n",
        "  #BN5 = tf.batch_normalization(A5)\r\n",
        "  #Max Pooling Layer2\r\n",
        "  A5 = tf.nn.max_pool(BN5, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  # FLATTEN\r\n",
        "  Flatten = tf.compat.v1.layers.flatten(P3)\r\n",
        "  Dense1 = tf.compat.v1.layers.dense(Flatten, units, activation=None)\r\n",
        "  Batch_norm1 = tf.nn.batch_normalization(Dense1)\r\n",
        "\r\n",
        "\r\n",
        "  output = tf.compat.v1.layers.dense(Batch_norm1, CLASSES, activation_fn=None)\r\n",
        "  return output\r\n",
        "  \r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "def initialize_parameters():\r\n",
        "    \r\n",
        "    W1 = tf.get_variable(\"W1\", [3, 3, 1, 32], initializer = tf.contrib.layers.xavier_initializer())\r\n",
        "    W2 = tf.get_variable(\"W2\", [3, 3, 32, 64], initializer = tf.contrib.layers.xavier_initializer())\r\n",
        "    W3 = tf.get_variable(\"W3\", [2, 2, 64, 128], initializer = tf.contrib.layers.xavier_initializer())\r\n",
        "    W4 = tf.get_variable(\"W4\", [2, 2, 128, 256], initializer = tf.contrib.layers.xavier_initializer())\r\n",
        "    W5 = tf.get_variable(\"W5\", [2, 2, 256, 512], initializer = tf.contrib.layers.xavier_initializer())\r\n",
        "\r\n",
        "    parameters = {\"W1\": W1,\r\n",
        "                  \"W2\": W2,\r\n",
        "                  \"W3\": W3,\r\n",
        "                  \"W4\": W4,\r\n",
        "                  \"W5\": W5,\r\n",
        "                  }\r\n",
        "   \r\n",
        "    return parameters\r\n",
        "\r\n",
        "\r\n",
        "def compute_cost(output, Y):\r\n",
        "\r\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = output, labels = Y))\r\n",
        "    \r\n",
        "    return cost\r\n",
        "\r\n",
        "\r\n",
        "def random_batches(trainx, trainy, batch_size = 64):\r\n",
        "    # number of training examples\r\n",
        "    no_samples =  len(trainx)               \r\n",
        "    batches = []\r\n",
        "    \r\n",
        "    # Shuffle (X, Y)\r\n",
        "    permutation = list(np.random.permutation(no_samples))\r\n",
        "    shuffled_X = trainx[permutation,:,:]\r\n",
        "    shuffled_Y = trainy[permutation,:]\r\n",
        "\r\n",
        "    # Partition (shuffled_X, shuffled_Y) and remove the end case.\r\n",
        "    num_complete_batches = math.floor(no_samples/batch_size) # number of mini batches of size mini_batch_size in your partitionning\r\n",
        "    for i in range(0, num_complete_batches):\r\n",
        "        batch_X = shuffled_X[i * batch_size : i * batch_size + batch_size,:,:,:]\r\n",
        "        batch_Y = shuffled_Y[i * batch_size : i * batch_size + batch_size,:]\r\n",
        "        batch = (batch_X, batch_Y)\r\n",
        "        batches.append(batch)\r\n",
        "    \r\n",
        "    # Handling the end case if last mini-batch < batch_size\r\n",
        "    if no_samples % batch_size != 0:\r\n",
        "        batch_X = shuffled_X[num_complete_batches * batch_size : no_samples,:,:,:]\r\n",
        "        batch_Y = shuffled_Y[num_complete_batches * batch_size : no_samples,:]\r\n",
        "        batch = (batch_X, batch_Y)\r\n",
        "        batches.append(batch)\r\n",
        "    \r\n",
        "    return mini_batches\r\n",
        "\r\n",
        "def model(trainx, trainy, testx, testy, learning_rate = 0.001,\r\n",
        "          num_epochs = 100, minibatch_size = 64):\r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    costs = []        \r\n",
        "     # Initializing parameters\r\n",
        "    parameters = initialize_parameters()                                \r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "     # Forward propagation\r\n",
        "    output = buildCNN(trainx,parameters)\r\n",
        "    \r\n",
        "    # Cost function\r\n",
        "    cost = compute_cost(output, trainy)\r\n",
        "    \r\n",
        "    # Backpropagation\r\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\r\n",
        "    \r\n",
        "    # Initializing all the variables globally\r\n",
        "    init = tf.global_variables_initializer()\r\n",
        "     \r\n",
        "    # Starting the session to compute the tensorflow graph\r\n",
        "    with tf.Session() as sess:\r\n",
        "        \r\n",
        "        \r\n",
        "        sess.run(init)\r\n",
        "        \r\n",
        "        \r\n",
        "        for epoch in range(num_epochs):\r\n",
        "\r\n",
        "            batch_cost = 0.\r\n",
        "            # number of batches of size batch_size in the train set\r\n",
        "            num_batches = int(no_samples / batch_size)\r\n",
        "            batches = random_batches(X_train, Y_train,batch_size)\r\n",
        "\r\n",
        "            for batch in batches:\r\n",
        "\r\n",
        "                # Select a batch\r\n",
        "                (batch_X, batch_Y) = batch\r\n",
        "                \r\n",
        "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={trainx: batch_X, trainy: batch_Y})\r\n",
        "                \r\n",
        "                batch_cost += temp_cost / num_batches\r\n",
        "                \r\n",
        "\r\n",
        "            \r\n",
        "            if epoch % 5 == 0:\r\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, batch_cost))\r\n",
        "            if epoch % 1 == 0:\r\n",
        "                costs.append(batch_cost)\r\n",
        "        # plot the cost\r\n",
        "        #plt.plot(np.squeeze(costs))\r\n",
        "        #plt.ylabel('cost')\r\n",
        "        #plt.xlabel('iterations (per tens)')\r\n",
        "        #plt.title(\"Learning rate =\" + str(learning_rate))\r\n",
        "        #plt.show()\r\n",
        "        \r\n",
        "        # lets save the parameters in a variable\r\n",
        "        parameters = sess.run(parameters)\r\n",
        "        print (\"Parameters have been trained!\")     \r\n",
        "        \r\n",
        "        # Calculate the correct predictions\r\n",
        "        predict_op = tf.argmax(output, 1)\r\n",
        "        correct_prediction = tf.equal(predict_op, tf.argmax(testy, 1))\r\n",
        "        \r\n",
        "        # Calculate accuracy on the test set\r\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n",
        "        train_accuracy = accuracy.eval({X: trainx, Y: trainy})\r\n",
        "        test_accuracy = accuracy.eval({X: testx, Y: testy})\r\n",
        "        print(\"Train Accuracy:\", train_accuracy)\r\n",
        "        print(\"Test Accuracy:\", test_accuracy)\r\n",
        "        \r\n",
        "        # Saving our trained model\r\n",
        "        saver = tf.train.Saver()\r\n",
        "        tf.add_to_collection('predict_op', predict_op)\r\n",
        "        saver.save(sess, './my-CNN-model')        \r\n",
        "                \r\n",
        "        \r\n",
        "      \r\n",
        "              \r\n",
        "                \r\n",
        "    return train_accuracy, test_accuracy, parameters\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}